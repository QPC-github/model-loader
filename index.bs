<pre class='metadata'>
Title: Model Loader API
Shortname: model-loader
Level: 1
Status: w3c/CG-DRAFT
Group: webml
URL: https://webmachinelearning.github.io/model-loader/
!Explainer: <a href="https://github.com/webmachinelearning/model-loader/blob/master/explainer.md">explainer.md</a>
Editor: Jonathan Bingham 114606, Google Inc. https://google.com
Abstract: This document describes an API to load a custom pre-trained machine learning model.
Logo: https://webmachinelearning.github.io/webmachinelearning-logo.png
</pre>
<pre class="anchors">
urlPrefix: https://webmachinelearning.github.io/webnn/; url: dom-navigator-ml; type: interface; text: ML
</pre>
<pre class="anchors">
urlPrefix: https://webmachinelearning.github.io/webnn/; spec: webnn
    type: interface
        text: ML; url: ml
        text: MLContextOptions; url: dictdef-mlcontextoptions
        text: MLContext; url: mlcontext
        text: MLNamedInputs; url: typedefdef-mlnamedinputs
        text: MLNamedOutputs; url: typedefdef-mlnamedoutputs
</pre>
<pre class="link-defaults">
spec: webnn; type: interface; text: ML
</pre>

Introduction {#intro}
=====================

For the introduction and use cases, please see the <a href="https://github.com/webmachinelearning/model-loader/blob/master/explainer.md">explainer.md</a>.

For illustration purposes, the API and examples use the [TF Lite flatbuffer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs) format.

API {#api}
==========

<pre class="idl">
enum MLModelFormat { "tflite" };

dictionary MLModelLoaderContextOptions : MLContextOptions {
  MLModelFormat modelFormat = "tflite";
};

partial interface ML {
  MLContext createContext(optional MLModelLoaderContextOptions options = {});
};

dictionary LoadOptions {
  // TBD InputNodes and OutputNodes
  required InputNodes inputs;
  required OutputNodes outputs;
};

[SecureContext, Exposed=(Window, DedicatedWorker)]
interface MLModelLoader {
  constructor(MLContext context);
  Promise&lt;MLModel&gt; load(ArrayBufferView modelBuffer, LoadOptions options);
};

[SecureContext, Exposed=(Window, DedicatedWorker)]
interface MLModel {
  Promise&lt;undefined&gt; compute(MLNamedInputs inputs, MLNamedOutputs outputs);
};
</pre>


Examples {#examples}
==================

<pre highlight="js">
// First, create an MLContext. This is consistent with WebNN API. And we will add a
// new field, "modelFormat".
const context = await navigator.ml.createContext(
                                     { devicePreference: "gpu",
                                       powerPreference: "low-power",
                                       modelFormat: "tflite" });
// Then create the model loader using the ML context.
loader = new MLModelLoader(context);
// In the first version, we only support loading models from ArrayBuffers. We
// believe this covers most of the usage cases. Web developers can download the
// model, e.g., by the fetch API. We can add new "load" functions in the future
// if they are really needed.
const modelUrl = 'https://path/to/model/file';
const modelBuffer = await fetch(modelUrl)
                            .then(response => response.arrayBuffer());
// Load the model. Notice that the indices for the input/output nodes are named
// and can be referenced in the "compute" function.
model = await loader.load(modelBuffer,
                            { inputs:  {x: 1, y: 2},
                              outputs: {z: 0 } });
// Compute z = f(x,y) where the output buffer is pre-allocated. This is consistent
// with the WebNN API and will be good when, for example, the output buffer is a
// GPU buffer.
z_buffer = new Float64Array(1);
// The "model.compute" function is async and returns an empty promise.
// Here we make the input/output format consistent with the WebNN API.
await model.compute({ x: new Float64Array([10]),
                      y: new Float64Array([20])},
                    { z: z_buffer} );
</pre>
